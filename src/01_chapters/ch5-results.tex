\chapter{Results and Discussion}

This chapter presents the findings of this work as well as the work that has been done and the challenges incurred and then discusses them in a wider context. The work regarding the configuration will first be evaluated upon its value and its design principles.

%Is this a sensible result representing reality and fulfilling demands in respect to working SHM and FAIR principles?

%Thoughts on data structure flow.





%\section{Metadata structure}

\section{Metadata system}

The Metadata plays a focal role within this work. It does not only just represent the data but is also vital for honoring the FAIR principles and opening this work up for further use. For completeness the parsing and upload will also be mentioned in this section since it all becomes part of the bigger picture. It also works up its way to achieve low coupling of software parts while maintaining high coherence of the metasystem behind, standardizing its single parts. Advertently achieving structured design as proposed by industry norms such as \textcite{stevens_structured_1974}.
For the data provision, a common ground has been found for the upload so that alterations to further processing parts of the cycle don't necessitate a full reupload of the original data. This common ground asserts that original data is uploaded once with the downside that the DAQ configuration may not be fully interpretable at this stage due not being self-describing metadata.

The following step of configuration solves this problem then by collecting and merging available metadata into a common scheme. This step works upon the DAQ metadata that is already uploaded to the Skystash and then fuses it with other metadata sources that are locally available to generate a holistically valid metadata set. Notable is that this step will certainly be part of rather constant modification due to SHM configurations that are also provided within this step. Also the metadata schema is far from complete and will be in need of constant expansion to compensate the complex dimensions of Flight Test Data as well as relevant test configuration data that also may be taken into consideration within this step \ref{fig:digecat_data_sources}. Since this step is far less computationally intensive than the parsing process this is considered as an acceptable tradeoff within this process.

\subsection{Results}


%• Config was developed according to the requirements and embedded into the previously existing system • Config for parameter properties in excel • Config merge performed to get one "true" config from DAQ+general config merge • Config for checks has also been implemented in a JSON-format


It has then be shown to be effective to separate metadata into the unmodified DAQ-data and a set of general metadata that provides all other necessary information. Be it for providing helpful details or vital SHM metadata. Fusing both sets of metadata then into a common set firstly elevates reliability of metadata by taking in account the DAQ-metadata. It then also guarantees interoperability and reusability by transforming metadata into a standardized metadata format that allows to be exchanged for the most effective format available since the transformation process takes marginal effort once the data is digitally available and compounded.
It has then also been adhered to the process standard by not inventing another new standard but respecting existing standards to use an excel sheet for configuration purposes. Using this approach and keeping information centralized, a later migration of data will be much facilitated.

Regarding the algorithmic part, the configuration has been created within a JSON-format. Reducing coupling while allowing a more dynamic expansion of the codebase. This makes it easier to implement changes and reduces the points of entry for making changes for creating new routines


\subsection{Discussion}
%• Good results. • Good adherence to the FAIR guiding principles (quick crosscheck)
%• However config editing in excel is pretty pisspoor from a version control standpoint • Has a lot of potential to become quite powerful as simple changes in config-setup don't influence rest of the toolchain • "low coupling --> high cohesion"

Adherence to the FAIR principles should be the main focus. Especially when handling metadata. The developed metadata handling can assert to have guaranteed the findability, interoperability and reusability within the metadata part of this work since the metadata clearly describes the data at hand (Findability), it then also clearly describes what attributes the data has and with which attributes the SHM algorithms will be run (Interoperability, Reusability) to then allow future users to reproduce and comprehend the full SHM process. In addition, the low coupling and high cohesion are central paradigms of this work.

The current form of the general configuration however poses some serious challenges and issues. It has the upside of being centralized but the downside of being in a not version-controlled environment. Not allowing backtracking and making search for faults very difficult. It also hinders development since experimental changes can only be made by copying to a local version that is then not version-controlled. It also does not allow dynamic changes to the metadata and in general does not adhere to the presented FAIR principles. However, once this issue has been fixed this work may bring more than marginal improvements to the world of data quality, allowing small changes of settings in the general configuration to trigger the SHM toolchain, updating data quality indices as you go. This metadata-workflow is also not limited to the  ISTAR data and may be applied to other aircraft or even machines or systems as well since the area of sensors is a constantly growing field. Even considering that in the IoT and IoP \cite{pennekamp_towards_2019} the actual sensor values are hardly ever checked. Since this is a growing topic this work attempts to lay a solid foundation upon which future endeavors may build upon and flourish.


\section{Algorithms}

The core component of this work are the algorithms to actually detect the errors. These Algorithms have been implemented to catch the major recurring errors that are known to be within the datasets. Although most often errors occur in Level 1 and become critical due to oversight, other error types are also important to consider. The sheer amount of ISTAR sensors voids any ability to maintain an overview over the entire system.

For trial and erroring and embarking on the SHM journey, the experimentally installed sensors in the ISTAR serve as a significantly better benchmark for errors since they are less reliable than the inherent aircraft sensors and thus also produce more interesting errors. Four errors from accelerometers and strain gauges are shown in Figure \ref{fig:error_plots}. Ranging from interrupted transmission to odd offsets and plain overflow values.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{python_functions/images/error_plots}
    \caption[Errors upon which the algorithms are trained]{The exemplary errors upon which the SHM algorithms were trained are shown above. Three accelerometers in different flights were showing faulty behavior. The plots on the left show a sudden interruption of sensor data. While the plot on the top right shows an unusual offset of the accelerometer since the parameter value should average at one. On the bottom right the strain gauge sensor was expected to produce an output similar to the other sensors and contain some levels of noise. It is, however, at the hard limit.}
    \label{fig:error_plots}
\end{figure}


\subsection{Results}


In the following, the outcomes of algorithms of Levels 1-3 are presented and henceforth discussed.
The validation errors shown in Figure \ref{fig:error_plots} are mostly solved since some strange errors make automatic detection difficult.
Beginning with Level 1, all  parameters that have not made it to the Skystash but have been expected to are filtered out. Acknowledging that Level 1 merely builds the first defense against errors this is a good start for the SHM, catching all errors that are plainly not available.

In Level 2 the most recurring faults are caught. It evaluates single sensor timeseries and possibly transforms them to then check against the limits defined in the configuration. In Figure \ref{fig:results_050303_spectrogram} the basic spectrogram for the top left accelerometer in Figure \ref{fig:error_plots} is given.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{03_figures/python_functions/images/FUS_050303_spectro.png}
    \caption[Level 2 spectral analysis of an erroneous accelerometer]{Spectrogram analysis of the erroneous accelerometer in Figure \ref{fig:error_plots}'s top left. In this spectral analysis the color shows the amplitude of the frequency that is normalized to 1. The error is then quantifiable since hard limits can be defined for the signal's amplitude.}
    \label{fig:results_050303_spectrogram}
\end{figure}

Based upon this spectrographic analysis outliers can be identified whose movement is either too high or too low. Using this spectrogram an averaged normed amplitude value can be calculated that represents the sensor movement. In Figure \ref{fig:results_050303_1} as well as \ref{fig:results_050303_2} the erroneous behaviors are clearly identified by falling outside the amplitude boundaries that have previously been defined in the general configuration.
Additional faults such as the strain gauge forming the bottom right value in Figure \ref{fig:error_plots} can quickly be identified since a regular value of such a sensor generally falls into the limits of $[-1000, +1000]$. This becomes obvious in Figure \ref{fig:results_230306} when comparing to the limits in red.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{03_figures/python_functions/images/FUS_050303}
    \caption[Using STFT and averaging to detect errors in Level 2]{STFT Analysis for a faulty acceleration sensor (top left in Figure \ref{fig:error_plots}).
    Since the value could not be filtered by simply checking the limits a new method has been devised using the STFT. As the averaged amplitude exceeds the red limits as shown in the bottom figure the then detected anomalies can get recorded for the SHM.}
    \label{fig:results_050303_1}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{03_figures/python_functions/images/FUS_050303_2}
    \caption[Using STFT on bottom left sensor from Figure \ref{fig:error_plots}]{For the second parameter at the bottom left in Figure \ref{fig:error_plots} the anomalous values can get detectd as well, proving the STFT approach to be successful on this limited validation set of sensor malfunctions.}
    \label{fig:results_050303_2}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{python_functions/images/DFUS_230306}
    \caption[Malfunction of the Strain Gauge]{The malfunction of the strain gauge (bottom right in Figure \ref{fig:error_plots}) is shown. Since the average operating limits lie between -500 and +500 the conservatively chosen limit detects all erroneous values.}
    \label{fig:results_230306}
\end{figure}

Detecting smaller, constant offsets that are within limits such as the accelerometer in z direction in the top right plot in Figure \ref{fig:error_plots} does not get filtered by Level 2 algorithms as shown in Figure \ref{fig:results_050209}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{03_figures/python_functions/images/FUS_050209}
    \caption[Analysis of the offset vertical accelerometer]{The analysis of the offset vertical accelerometer shows that this offset is not very easy to detect without any additional sensor-proprietary information. However, a Level 3 approach in further development may be able to detect such an offset by determining the fused rigid body acceleration of the ISTAR in z-direction and detecting that the acceleration values in this figure obviously strongly differ from other acceleration sensor in z-direction.}
    \label{fig:results_050209}
\end{figure}

To detect an offset in z direction it is more sensible to include other correlating z sensors or the rigid body model of the aircraft. Determining in acceleration in z direction within such a model should allow correlation of flight angles as well as other accelerometers in the Inertial Measurement Units (IMUs) as well as relational angles indicating turning of the aircraft and hence more acceleration in z-direction.
\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{stashboard_level3_zoomed}
    \caption[Level 3 SHM for the altitude-dependent parameters.]{The Level 3 calculation for barometric altitudes with a the tolerance area shown as the transparent blue area are displayed. Since the allowed deviation is calculated automatically it is quite generous with about 100m. This is however far enough to determine that the static pressure measured by the nose-boom is out of bounds for the entire time of this recording session.}
    \label{fig:stashboard_level3_zoomed}
\end{figure}

For now, however, only the altitude with the specialization of the barometric altitudes has been implemented. In Level 3 an automatic limit gets calculated based on the fused signal. Here, the fused signal gets detrended to remove any trends and the standard deviation of the signal gets used to measure expected deviations. The fused signal combined with the expected deviation is shown in Stashboard Figure \ref{fig:stashboard_level3_zoomed}.

Additionally, Level 3 serves as an additional layer of safety against errors. It also is possible to detect errors with a higher precision compared to the Level 2 algorithms.

%todo: show highpassfiltering and signal extraction



\paragraph{Choosing check parameters}

Automatically setting boundaries for Level 2 has proven to difficult due to the sheer difference of sensor behavior available. Hence, the automatically set limits for Level 2 were abandoned and replaced by hard limits that needed to be set in the general configuration. However, automatically determined boundaries were still employed and shown to be working for Level 3. Using the detrended signal has proven to deliver sensible limits as shown in the Stashboard in Figure \ref{fig:stashboard_level3}.

However, setting limits for Level 2 still requires some trial and error to reliably work for sensors with the validation loop being driven by previously detected error types. It shows that while conservatively chosen ranges do not indicate many occurrences, tighter limits enable more detailed monitoring. Choosing a range still need some tuning since the limits at Figure \ref{fig:results_050303_1} seem safe but for Figure \ref{fig:results_050303_2} seem very close to detection. Thus, still needing further investigation to safely predict errors. Possibly implementing other type of sensor movement analyzes to deliver results that are more reliably and more tailored to various sensor.

Another interpretation of the sensitivity in analysis methods lies in too tight tolerances. These may detect every small aberration but generate too many occurrences when actual malfunctions are of interest and not every single perturbation. Due to limited time this algorithm could not be examined on its performance for other datasets which remains open for future work.


\subsection{Discussion}
The results from the algorithms speak for themselves. Although the entire configuration set is not complete and the system is in prototype stadium good results have been achieved. Most of the required errors and malfunctions shown in Figure \ref{fig:error_plots} have been identified by the SHM developed in this work and even if these algorithms may not suffice more complex errors, the overall architecture makes integration of new algorithms for quick deployment easy.
%todo: show for which errors this has been achieved
The results in Level 1 are primarily satisfying the application's needs. However, additional interfaces could be implemented displaying sensor subsets in a hierarchical view similar to Level 3 Stashboard implementation (see Figure \ref{fig:stashboard_level3}). This could facilitate interpretation which subset of experiments is currently performed. E.g. whether the Nose boom of the ISTAR is presently used based on the given data and metadata.

Level 2 algorithms worked well on the posed problems and also detected complex errors such as the accelerometer faults that resulted in a very little amount of noise. Using Level 2 approaches enabled detection of most known errors, providing a substantial foundation for the proceeding work on ISTAR data. Implementing new functions is realized by using pandas series and dataframes for function in- and outputs and modifying the SHM JSON-configuration.
The FMEA developed also paves the way for further advancements such as a real-time implementation that may be used during experiments to detect possible sensor malfunctions based on processes that are computationally frugal and are able to run in real-time.
%todo: level 3 performance eval?
%Wie bei den vorherigen beiden Kommentaren. Ich weiß, dass das insbesondere bei Schritt 3 sehr schwierig ist, weil es im Grunde keine "Ground-Truth" gibt. Gibt es trotzdem eine Möglichkeit wie die Performance von den Parity Functions zumindest grob evaluieren aber konkret kannst?
%
%Außerdem gehst du hier mMn zu sehr auf die Nachteile deiner Implementierung ein und das hat deine gute Entwicklungs- und Implementierungsarbeit wirklich nicht verdient! ;-)
%Versuche die positiven Aspekte stärker in der Vordergrund zu rücken.
Parity Equations enable correlation of parameter behavior even though they present a prototypical beginning for FMEA implementations in the SHM. Neglecting the limitations of Parity Equations they still provide value and truth for correlation making them indispensable in their function as white-box models. The Parity Equations developed in this work still has potential. It may be developed further to include a rigid body model of the aircraft, correlating various axes and states to finally achieve a minimal model error.
Applications in other fields might also find this approach useful. Since the configuration has the only constraint of being hierarchical other physical relations such as in industrial applications can also be represented once these physical relations are mapped to python functions.
Considering further possibilities for Level 3 FMEA various approaches could be assessed and implemented. These would be methods such as Kalman Filtering \cite{lie_synthetic_2013}, PCA \cite{isermann_fault-diagnosis_2006} and other analytical approaches \cite{freeman_air_2013, perhinschi_integrated_2010}.  Exceeding the conventional FMEAs presented in Figure \ref{fig:FMEA-methods}, novel methods ranging from model driven designs such as the Pseudo Transfer Function approach \cite{aljanaideh_aircraft_2015} up to state of the art data-driven models for image processing such as GANomaly \cite{akcay_ganomaly_2018} could be implemented. These could be modified for use in research data using latent spaces to identify principal state parameters as well as identifying possibly new hidden correlations.

Within the algorithmic step it has been endeavored to comply with the Interoperable and Reusable criteria of the FAIR principles. Implementing this by using JSON-configurations in standardized formats when possible and decoupling the actual algorithmic from the data communication by using pandas as the interface for the python functions.

%todo: comment FAIR
%Die FAIR Prinzipien beziehen sich eigentlich nur auf Daten selbst und weniger auf Algorithmen. Da Source Code selbst aber auch Daten sind, passt das schon.
%
%Mir fehlt allerdings der FAIR Aspekt bezüglich der Datenstruktur in der du die Ergebnisse im Stash speicherst. In Bezug auf die spätere und langfristige Verwendbarkeit der Daten ist das der entscheidende Punkt. Da kannst du doch mehr zu sagen als "wir verwenden JSON", oder? Wie sieht die Datenstruktur aus, welche interpretierbaren Keys werden verwendet? Dieser Teil ist allerdings eher was für Kapitel 4. Hier wäre dann eine Evaluierung der gewählten Struktur passend: Warum ist die Struktur FAIR?

\section{Visualization}
An interactive frontend for the data quality report has been developed to enable accessibility of the SHM. Visualizing the data quality for an entire flight dataset at a single glance. Searchable metadata tags enable a dynamic search within the Skystash database and due to its web based architecture it can be deployed as a webservice to facilitate usage.

\subsection{Results}
As mentioned in \ref{chap:4-visualization}, the Stashboard allows rapid SHM prototyping and quickly generates an overview over ISTAR flights' data quality, swiftly summarizing the large datasets. It interfaces with the Skystash over its python Application Programming Interface (API) and using the JSON-report data it is an extension module to provide a Human-Machine Interface (HMI) for the SHM.

The Timeline graph serves as a starting point to detect sensor failures that are associated with sensors outputting invalid values and thus having strange start- and end-times of their respective measurement. An advantage of this display is that it works out of the box on all Skystash flights  since it accesses automatically generated Skystash data, not needing any processing and SHM.

The Level 1 implementation currently is available in the shape of a gauge, indicating the percentage of available sensors. A helpful addition for later work could be the aforementioned visualization of sensor groups in hierarchical overview. Perhaps color mapping single sensors and sensor groups to their availability or other relevant properties.

For Level 2, any algorithms may be dynamically added to the existing configurations in the data processing step. Integrating this works without troubles since the JSON-report allows dynamic extensions on Level 2 attributes.
In Level 3, it has been achieved to compare similar sensors that represent the same state variables by representing sensor relations within a tree-structure, the fused signal of all sensors with its automatically detected limit and overall sensor offsets to detect any strong biases of sensors.

\subsection{Discussion}

This dashboard implements a quick overview and architecturally attempts a low coupling high coherency to the Skystash and the other parts of the SHM-toolchain, having the Skystash API serve as the only interface needed to operate the Stashboard.
Regarding FAIR guidelines, this step fulfills the Interoperability step, allowing any flight with a SHM-report to be loaded and displayed, lowering the barrier of entry for entering the world of data quality. Findability of data is then additionally guaranteed by providing a tool to check any dataset for its inherent data quality, facilitating data discovery by contributing a new method to clearly interpret data quality.
For future expanses, new data quality visualizations and overall displays allow to be implemented in the dashboard structure. Also necessary to maintain overview would be aircraft landing page superset of the current dashboard. This method then may give a better overview over the entire aircraft lifespan and the reliability of its employed sensors and their failure rates. Additional implementations may add various analytic tools for the Flight Test Instrumentation Group to further analyze data such as the TerraWarn-Program, currently being developed at the FTI-group, analyzing flight performances for internal debriefing purposes. Other methods could be a Flight Data Player that allowed to replay the flight at any moment, generating data-quality analyzes for that specific point in time. Overall, this work provides a fundamental base for further Skystash analytic tooling in the Digital Twin project, honoring the FAIR guidelines.


\section{Integration of the application}

Summarizing the development, the integration into the Skystash ecosystem builds the foundation for a novel data space for Flight Test Data. Following Figure \ref{fig:fti_microservices}, all tools are dependent on the Skystash and serve as extensions for the existing architecture, expanding it to elevate data interpretability which is vital in the jumble of Research Data available. The programming standard of low coupling but high cohesion \cite{stevens_structured_1974} leverages the Skystash API and allows for reduction of technical debt in the future, making maintenance and expansions on single components less complex, reducing interactions.
The overall evaluation of FAIR principles is shown in Figure \ref{fig:FAIR_evaluation}. Starting from the upload to the Skystash. The genuine DAQ data and metadata is preserved. Metadata Enrichment then decodes the DAQ configuration and assigns extended information to the DAQ metadata and enhancing the knowledge further by also adding SHM-configurations. Within the following data processing step, all metadata for calculations is already available and the processing step can run without any interaction and also allowing backtracking based upon the enriched metadata that provides the data processing configuration. Finally, the report is uploaded to the Skystash. It then serves as an indicator for the data quality which can then get interpreted by the data visualization tool which allows quick interpretation of the generated report.


\begin{figure}
    \centering
    \includegraphics[width = 0.6\textwidth]{FAIR_evaluation}
    \caption[FAIR-correlation matrix for the SHM-]{Correlating parts of this work by rating their adherence to the FAIR guiding principles. Shown are the icons describing the singular decoupled packages introduced in Figure \ref{fig:SHM_simplified} and \ref{fig:fti_microservices} that work by interacting with the Skystash architecture (see Chapter \ref{chap:skystash})}
    \label{fig:FAIR_evaluation}
\end{figure}

\section{Summary}
The past chapter has shown the outcomes of this work and discussed tradeoffs as well as successes followed by opportunities for expansion which could be realized in future work.
This work delivers a prototype for a Sensor Health Monitoring Infrastructure. It establishes the SHM-process as a holistic toolchain of singular methods to enable future expansion by design.



