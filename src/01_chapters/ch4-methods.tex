\chapter{Methods}

The implementation efforts of this thesis are condensed into the following chapter. A holistic perspective on the data process is described in the following step following from the urgency that Sensor Health Monitoring needs to be perceived within its ecosystem. A SHM thus is directly reliant upon the data in the first place. It is then also reliant upon the configuration metadata since it will contain necessary information for processing the data further within the SHM. To then close the feedback loop it is also vital to generate a sensible data display that allows quick evaluation of the SHM since manual evaluation methods would slow the development process by orders of magnitude.

And: The implementation and methods are compounded to reach a thematic

But:


Therefore


\section{Introduction}


The implementation can be detailed by figure \ref{fig:fti_microservices}. After having uploaded the Dataset the relevant information on the sensors as well as the aircraft properties is gathered and condensed into a single configuration metadata-set represented by a JSON file. This JSON file is then appended to the Dataset within the skystash architecture represented by the Online side that is accessed by the API. Processing the data now becomes a clean blackbox operation that is only fed by data received by the API returning its report containing notable occurences via the API and storing all relevant information online. Since this software is developed for flight operations and an interactive visualization of the report data facilitates evaluation of the generated report data a User Interface within a dashboard application is developed. Additional benefits contrary to the generation of PDF reports are dynamic updates as well as interactivity and an agile update cycle considering the spontaneous emergence of bugs.


\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{FT_microservices_AWS}
    \caption{The data toolchain prospectively used for Sensor Health Monitoring}
    \label{fig:fti_microservices}
\end{figure}


\section{Data Parsing}
%Since the SHM has to run on something, the original data is briefly mentioned for completeness.

Since the data the SHM will work on orginates from the airplane and needs to uploaded without any modification to preserve originality this small section will deal with the handling of data from the aircraft until it reaches its destination in the skystash.

The Data is uploaded to the stash from the aircraft DAQ format. Its data is available in the proprietary IMC .raw formats. These are then parsed into the stash by converting them into timeseries for each parameter. The data structure used in the skystash is represented in figure \ref{fig:skystash_folder_structure}. The folder structure's main path is the project folder which represents the project in which the flight was performed in and contains all the project's flights. Each flight then contains parameter measurements which represent the lowest level of objects in the architecture. All objects of project, flight or measurement type can also contain JSON-usertags as well as additional data in binary format such as pdf files.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{skystash_folder_structure}
    \caption{skystash folder structure}
    \label{fig:skystash_folder_structure}
\end{figure}

\subsection{Parsing of ISTAR Data}
%ISTAR data originates from the ISTAR's DAQ system in the shape of .raw files for each parameter and is parsed and uploaded to the stash. It is then accessible via the stash api and can also be inspected in the stash webview. After the Dataset generation step the metadata available is only the one from the ISTAR DAQ system.
%The data is present in the shape of Projects->Collections->Series


To provide a full overview of the entire pipeline this step is mentioned. Starting off, we are left with a directory full of .raw and .imcexp files after a flight. Both are proprietary IMC formats. However, we can distinguish between both insofar that .raw files contain timeseries in an encoded format. The imcexp files then contain the configuration settings for the specific flight/measurement.

The entire directory then gets parsed by a python script. Segmenting the binary structure of the raw files and extracting the timeseries for each parameter. The DAQ and aircraft sensor config for each flight gets exported from the imcexp file and uploaded to the skystash without modification to preserve genuineness of the data as shown in figure \ref{fig:skystash_data_pipeline}. Any Metadata in shape of a JSON format is represented with a red dot and any binary formats such as pdf files or other custom data are represented with a blue dot.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{skystash_data_pipeline}
    \caption{Pipeline of parsing and uploading the skystash data to the skystash without modification}
    \label{fig:skystash_data_pipeline}
\end{figure}

Parameters are saved under the flight object. Config data is assigned to the user tags of each parameter. The data is then parsed for each flight and inherent sensor.

The metadata is currently only the one from the ISTAR DAQ system. Since this metadata needs further information to make it understandable it will be enriched further within the Configuration step (See figure \ref{fig:fti_microservices}).

\newpage


\section{Configuration-Metadata}

The configuration from the previous step has now been uploaded to the skystash. It describes the actual flight-dependent DAQ configuration and changes for every flight. It is, however, not complete. Since the DAQ only saves data relevant to itself, other vital metadata for users as well as the SHM are not present. In this section a method is implemented to enrich the DAQ metadata by transforming it into a common metadata format as well as appending useful additional metadata to each sensor and the flight.

%-This step details the considerations and implementations taken to be able to generate a useful metadata model that applies necessary information to the dataset within the skystash.

\subsection{Why?}

%§ Metadata from DAQ sensible and up-to-date but incomplete
%-incomplete metadata
%-Metadata is not complete yet. -does only contain info from DAQ which represents only the info the DAQ needs. What about all the other information that is not in the DAQ?

The DAQ configuration in itself is very essential but also incomplete in that is does not yet contain necessary information for detecting errors within our data. To achieve this we need to enhance our metadata from the DAQ with additional data. This permanent, generally unchanging metadata is first specified locally and contains values such as boundaries for level 2 or tags for level 3 to correlate parameters to each other. It is also specified for each parameter individually. It is then also possible to assign values such as frequency locally.


%§ SHM-info not available in DAQ-metadata, e.g. positional of sensors not available -shm info necessary

This necessitates a merging/fusion step of DAQ-config with the locally defined permanent metadata. This is especially useful since this locally defined metadata is not up-to-date and DAQ metada is not complete. It also Necessary to additionally add data relevant to the checks for each parameter.

%§ Input needed for SHM with all necessary infos

\subsubsection{Requirements}
%What is the Goal?

%§ Enrichment of Metadata
Leveraging these requirements we can define two goals for the configuration step.
\begin{enumerate}
    \item Enrichment of Metadata
    \item Conversion into a standardized format
\end{enumerate}
These two steps guarantee a similarity across processes which will also pay off once the SHM process may expand to other applications. The standardized format also supports the desire for FAIR principles since it allows to input data from various sources into the SHM data processing algorithm.

Additional attributes that are needed are aircraft specific information such as position of sensors within the aircraft Coordinate System (COS), the actual position of the COS relative to the aircraft and its orientation as well. For the SHM Level 2 physical limits as well as amplitudes for STFT are needed. For Level 3, only the parameter tag, defining its meaning in the application context such as \textit{static pressure} and an eventual reference parameter is defined. The reference defines context or calibration and defines the \textit{reference pressure} in the case of the \textit{static pressure}.

Generally, metadata exists in two forms. the actual flight-dependent DAQ config and the permanently valid metadata config that principally does not change. The actual config is different for each flight since it represents changes made by Flight Test Engineers to fit the DAQ-configuration the specific flight experiment. The permanent config however represents all additional info that may be relevant to a varied pool of users.


%§ Conversion into a standardized DAQ-independent format
Once a merged data construct is reached it needs to be converted into a usable, standardized format. For this step, some candidates from Internet of Things (IoT) applications have already been examined in \textcite{bodenbenner_model-driven_2022}. For the pipeline to the data processing the metadata can then be transformed into the SensOr Interfacing Language (SOIL), allowing a concise, specified format for this essential metadata.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{skystash_metadata_pipeline}
    \caption{Metadata Pipeline}
    \label{fig:skystash_metadata_pipeline}
\end{figure}

\subsubsection{Implementation}
%implementation
%\\§ Accumulation of required metadata that needs to be supplemented
Required data then needs to be defined locally. For this, an already existing database is taken as a starting point. This database is in the shape of an excel file and will be used for development. In further steps it might be reasonable to migrate this database to a different format to guarantee true provenance for configuration information. However, this is reserved for a later point. It suffices for now and will be expanded for columns that can add attributes for each parameter such as the aforementioned level 2 and 3 requirements and also additional data that may be of interest for users in the skystash such as sensor position in the aircraft. This is shown in figure \ref{fig:skystash_data_pipeline} within the config and metadata elements that represent permanent config info and the metadata for the steps.

The metadata fusion is part of the feedback loop that is in-line with the data processing and feedback. So this step is essential to the process since previously all steps from the SHM-toolchain figure \ref{fig:fti_microservices} had to be performed previously if there had been any marginal changes. Compartmentalizing and modularizing these single steps allows to then accelerate development and testing by not having to parse data within each iteration.

%-The configuration of the ISTAR is of great importance for the data processing. Also of great importance is the knowledge database which is currently in the shape of an excel document.
Also, attributes that are generally transmitted within the DAQ configuration are sampling rates, data origins and other miscellaneous information related to the system. Not contained are information about general sensor description, position of sensors, overall setup of the aircraft sensor architecture and check limits for data values. To preserve the original DAQ infos as closely as possible the merging step as portrayed in figure \ref{fig:skystash_metadata_merging} is proposed. This way, original DAQ info that is frequently changing such as the frequency gets preserved while still allowing additional data by the permanent local config to be used

%\\§ Fusioning of Metadata with Priorization of DAQ data
The metadata enrichment process is described in Figure \ref{fig:skystash_metadata_pipeline} starting with data and metadata that is available in the skystash and unmodified. This unmodified data then gets enriched with metadata for parameters that are not already described by the DAQ config. In this step other SHM attributes get assigned such as checking limits for level 2 and variable tags for level 3 defining the meaning of a parameter. Since the metadata uploaded to the stash in itself is not very expressive and cryptic it needs to be refined further into a usable, concise format.
\begin{figure}
    \centering
    \includegraphics{skystash_metadata_merging}
    \caption{Priority Merging of Skystash data. Using the actual-DAQ configuration to overwrite other metadata.}
    \label{fig:skystash_metadata_merging}
\end{figure}


%\\§ Then translation/conversion into standardized SOIL format
As previously discussed in chapter \ref{chap:2-metadata-format}, a common format for exchanging metadata is needed. Following Fusion, the data is now available in a JSON format that contains the previously defined attributes for each parameter. To increase standardization a further conversion into the SensOr Interfacing Language is considered. This could increase acceptance and standardization since it is starting to be increasingly used within IoT contexts. A proposed structure would contain single sensors as parent elements and components containing the appended attributes. \cite{behrens_domain-specific_2021}

\subsubsection{Summary}
%§ Generating additional metadata
A central format for metadata has now been agreed upon. Especially data flows have been determined which strongly facilitates further development by structuring input and output operations. Primarily, the place to define further metadata attributes has been defined and the step to compile it within the configuration data merge step. Naturally, this will have to be changed further within further development. This method however condenses work into one single place.
%§ Fusion of metadata sets
The incomplete metadata from the DAQ has been enriched with the locally available metadata such as additional knowledge about the aircraft, the sensors itself and its context. Also, necessary data for the SHM step has been added such as limits for checks as well as tagging parameters for physical correlation checks which has been fused into a single data structure.
%§Conversion into standardized format
After compiling various inputs into a single metadata structure a common format has also been found. SOIL has been examined for usage and implementation is intended for future use.

\newpage


\section{Data Processing}
Now, we can sort out the vital step of this work. Taking the inputs of data and metadata with the configuration needed and implementing sensible logic to reliably detect anomalies in the data. This will first be preceeded by some considerations about software architecture and then delving into the specific methods for FMEA.

\subsection{Software architecture considerations}
%todo: check mention of :				□ Modularity	□ Modifiability □ Reusability □ Quick Feedback


Careful consideration needs to be given to the workflow of the level 1 check to allow scalability and reduce manual interaction. To achieve this architecture manual steps are reduced as far as possible. However, some level of configuration must be implemented otherwise only relational sensor behaviour could be detected. Meaning that sensor faults occuring temporarily within an experiment can be detected but permanent behavior is not noticed by a relative algorithm e.g. only detecting strong aberration. To satisfy this desire for information the preceeding metadata configuration step in chapter \ref{chap:2-metadata-format} is implemented.

\subsection{Check 1 Implementation}
For implementing the level 1 checks, the measured data needs to be compared to the expected data. To gain insight into what the the expected data is, the configuration file of the data acquisitioning system needs to get parsed as presented in chapter \ref{chap:2-metadata-format}.

It is possible to export the DAQ flight-specific configuration within the software \textit{IMC Studio}. This reads .imcexp files and then allows export into a .xml format. Downsides are though that various manual steps are involved in this process and it is thus prone to errors. So, we naturally attempt to automate it.

It turns out that the DAQ's imcexp format is in fact a .zip directory. The 7zip command line tool allows opening the configuration file since the configuration file's format is not fully complying to the zip standard making several python libraries fail during the process. This stems from an issue with the zip header and footer parts of the file that are not at expected places (effectively in the front and back). Once opened, the configuration file contains multiple files as well as an essential xml file that contains the needed sensor metadata. After some formatting these .imcexp can be read straight to generate similar info to the \textit{IMC Studio} workflow.

The expected data then can be extracted from the .imcexp files and allows quick comparison against the actually generated data. This resolves the Level 1 implementation.

\subsection{Check 2 Implementation}

Within Level 2, the sensor timeseries get checked by themselves without contextualization of other sensor data. This gets accomplished by principally checking them against predefined limits as shown in figure \ref{fig:level_2_flow}.

Use Cases for this are primarily the two implementations:

\begin{enumerate}
    \item unmodified signal
    \item noise amplitude by using STFT amplitude \ref{chap:2-plausibility}
\end{enumerate}

\subsubsection{Methods}

The general workflow consists of getting the timeseries of the signal from the skystash database, then transforming it in some respect and then checking it against limits.
%how it works
%• How does it generally work?
%□ It checks all parameters against a range and notes the values and timestamps where limits where exceeded.
%□ It then saves them into a json file for the type of check that was performed (value out of limits, movement out of limits

\begin{figure}
    \centering
    \includegraphics{level_2_flow}
    \caption{Checking Logic for every type of level 2 function.}
    \label{fig:level_2_flow}
\end{figure}

The logic in this step can then generally be summarized in equation \ref{eq:level_2} with $l$ being the limit that is checked against.
\begin{equation}
    f_{report}(t) = \left\{\begin{array}{rcll}
                               t:True & \mbox{for} & f_{value}(t) < l_{min} \vee f_{value}(t) > l_{max} \\
                               None   & \mbox{for} & l_{min} < f_{value}(t) < l_{max}
    \end{array}\right
    \label{eq:level_2}
\end{equation}

The context $f_{value}(t) = g(f(t))$ is additionally used. Representing any transformation operation as the function $g$.

\subsubsection{Invalid value}

A check implementation counterchecking the sensors sampling that generates fault detections based on deviation from predefined sampling. This takes into account the DAQ config since it contains the current sampling rate. Minimizing false positives.

This method is necessitated by the simple absence of \textit{None} or any kind of invalid values in the skystash. Meaning that such values are simply not present. To detect \textit{None} values we then are presented with two options:

\begin{enumerate}
    \item check parameter start and stop time
    \item check sampling rate
\end{enumerate}

We could also check for values remaining unchanged for longer times. This will be part of the section of \textit{sensor movement} mentioned below.

\subsubsection{Out of range}
Generally, predefined limits are checked within SHM Level 2. For the first step, values are checked whether they are within a predefined range. This means ranges such as defined in Table \ref{tab:level_2_range}. Of course, this selection is biased and may not be accurate for most mission profiles like atmospheric research aircraft that cruise in altitudes of up to 45,000 ft MSL.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
    \centering
    \begin{tabular}{@{}lll@{}}
        \toprule
        Parameter           & Minimum & Maximum   \\ \midrule
        Barometric Altitude & -1000ft & 40,000ft  \\
        Static Pressure     & 1Pa     & 120,000Pa \\ \bottomrule
    \end{tabular}
    \caption{Exemplary limits for checking parameters}
    \label{tab:level_2_range}
\end{table}

\subsubsection{Movement check}
\label{chap:4-level_2_movement}
The next category examining the sensor behaviour can be classified into sensor movement being too low or sensor movement being too high. Movement being defined within this context as the difference of a new sensor value to the previous one.

Hence, an approach using a STFT is chosen in which the amplitude is averaged across all frequencies from its spectrum. This guarantees a generalized feature extraction, resulting in standardization for parameters. Based on this spectral analysis, the logarithmic order of the variable can be estimated within its dataset. Previously, the frequency is assumed via time difference of start and end time divided by $n_{datapoints}$. The STFT also implements preprocessing steps that would otherwise be necessary such as translating the signal by its average value as well as scaling it by its standard deviation. Would one be interested in examining a signal using functions from a statistical toolbox, a similar result may be achieved by performing the previous steps of averaging and scaling the signal and then examining the variance within a moving window of the signal, similar to the STFT. The size of the moving window for such an analysis is defaulted to 256 data points for each signal. Certain issues with methods of moving windows are however, that they are not very meaningful for the beginning and ending of datasets within which the window is not fully occupied. This may lead to spikes for the STFT, which is why it is found that the STFT is generally more powerful to examine insufficient sensor movement.

This can be described in equation \ref{eq:level_2_movement_check} as:
\begin{equation}
    g(f(t)) = \frac{1}{n}\sum_{m}^{n} F(m,t)
    \label{eq:level_2_movement_check}
\end{equation}
\begin{conditions}
    m                          & frequency                                 \\
    n                          & amount of frequency points                \\
    X(m,t) = \mbox{STFT}(f(t)) & the STFT of f \cite{smith_scientist_1999} \\
\end{conditions}

\newpage

\subsection{Check 3 Implementation: }



I lied about single source of config. Level 2 and 3 also have their own config.

avoid weighting by structuring checks into a tree format for independent aircraft state variables.


Aims of Level 3 Implementation are to model the aircraft's state in a reference system. This reference system shall be geodetic and fixed to the earth while the aircraft moves through it. Moving aircraft coordinate systems like the aerodynamic and the along track Coordinate system may be derived from its geodetic position using ?angles and velocities.


Interfacing: Clean interfaces are generated throughout the model. Enabling a standardized state vector x. Meaning that A remains standardized for any aircraft while B, U and L need to be adjusted for any changes to the aircraft or sensor data.

The integration step may be omitted in early design stages since the necessary equations and equilibriums of Forces and Moments could only be modelled linearly while neglecting various unknown factors like shifting CG due to fuel burn, actual Inertia of Aircraft and aicraft mass. Hence, this step may be implemented if time allows it.

\subsubsection{Finding a reference state}

% todo: An aircraft position and state can be interpreted in various Coordinate Systems. To calculate a model it is first important to determine a single state that may be the principal coordinate system from which all other coordinate systems may be derived.

% cite flugdynamik skript

Necessary to compare parameters is a common reference system. Hence, reference systems are examined upon suitability for usage of parameter transformations.

A starting point which shall be considered is the geodetic reference. it measures the aircraft's position by its displacement from the previously (ref?) discussed WGS84 system. Meaning that altitude gets measured as the orthometric height (see ellipsoid height-geoid height).

Latitude and Longitude form the x and y axis of the COS. True heading forms the reference heading within the geodetic system.

All aircraft parameters related to motion and position of the aircraft should be attempted to be condensed into this form.

\paragraph{Dynamic configuration and correlation of parameters}

Motivation: a dynamic description for parameter correlations is searched in order to implement an efficient and quick way to assign parameter correlations. It is then assumed that a tree structure is fit for this task since parameters can be correlated to other upstream parameters in a single flow direction.


To attach meaning to the parameters, they are specifically tagged with descriptions of their function. A simple example that is implemented is that of \textit{static pressure}. The tag is defined in the excel configuration and furthermore gets configured into a dynamic configuration JSON-file. This file allows specification of sensors into subclasses and divides the aircraft into its degrees of freedom. This tree structure is then parsed and calculates a value for each level of detail taking into account its lower lying neighbours. Some weighting also has to be considered since a number of redundant pressure sensor could skew results against a single GNSS parameter. Thus a condensed parameter is calculated for each degree of freedom that is based on predefined algorithms and tags that can be freely defined within the config file that are then recursively parsed within the tree structure.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{level3_config}
    \caption{General structure of physical correlation setup}
    \label{fig:level3_config}
\end{figure}


In figure \ref{fig:level3_config} the general structure of the dynamic configuration in JSON format is shown. The configuration parent is an element similar to the configuration in SOIL-format \cite{behrens_domain-specific_2021}. In our case the ISTAR-aircraft. The istar aircraft contains top-level components that are the independent state variables of the aircraft. Generally a component can contain other components or parameter tags that describe parameters that are referenced. A component can also contain a function that transforms its parameters into the parent parameter.

\paragraph{Residual generation}
Based upon this previously defined model that is based on the aircraft configuration as a tree structure, residuals are generated based upon the difference of the parameters to the top level parameters (Single, fused value) and the single transformed sensor values (All Parameters) as previously discussed in Table \ref{tab:states_and_signals}.



\paragraph{Residual Interpretation}


The residuals are then checked against the limits of the standard deviation of the normal signal for a highpass filtering generated by taking a moving window of 256 points. If any value falls outside these values of the standard deviation of the condensed signal it is noted within the report. A fixed residual limit as well as a probability density function implemented in works such as \cite{svard_data-driven_2014} may be implemented at a later point.

\subsubsection{Examining Altitude Sensors}

As seen in~\ref{fig:gps_diff} gps altitudes have a base mean value of difference of about 3-4 meters. During flight level changes the base level changes significantly. Further investigation is needed how these changes may arise.

One possible approach would be considering different placements of gps antennas within the airframe. since the IMAR's position is precisely known and lies around the center of gravity but the ASCB's gps position is not known exactly the process is not facilitated. However using the process of lever arms the position could be roughly estimated and compensated from the altitude difference in figure \ref{fig:gps_diff}.

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{gps_difference_imar_ascb}
    \caption{Comparing the GPS sensors from the internal aircraft GPS (ASCB) to an experimentally installed GPS system (IMAR)}
    \label{fig:gps_diff}
\end{figure}

Another incurring deviation is investigating the ~40m/150ft offset for gps altitudes. Possible causes may be Uncorrected Ellipsoid gnss altitudes. However, this appears unlikely since generally the offset in the region would be added and not subtracted [further investigation needed].


Difficulty diagnosing sensors without previous knowledge. limitations within sensor behaviour. Checks include: range, too much sensor movement, too few sensor movement.

Relative examinations possible for errors occuring for finite time within experiment but not if all of the experiment data is corrupt.

\paragraph{Altitude Comparison}

plot here: -gps to gps -gps to baro. Beta overlayed as well as Ma Number


some mean ground has to be found to crossreference the various altitudes.

For standardization. The SI-unit meters is used for altitudes.

In the first draft. The altitude is sampled with 1Hz for computational speed and since gnss update rates are updated in a similar frequency.

\paragraph{ Physical}

\paragraph{ GNSS}

The following briefly glosses over GNSS and does not delve into the technical details. it rather tries to present GNSS Systems as a black box and examines inputs and outputs.

Generally speaking, GNSS systems calculate position based on run times to satellites. For output GNSS systems return position values for the ellipsoid model of the earth. Since the difference between MSL and ellipsoid varies for up to 100m vertically based on the receivers positions on the earth. Mostly, the difference is in the 40-50m range in germany.

Explain here why orthometric and ellipsoid value differs (vector difference, cg)

GNSS-units work with geoid models to deliver an orthometric value. Different Geoid models have different drawbacks and resolutions. For aviation use, the World Geodetic System (WGS84) generally proposes a geoid model that is used. Within this works scope it is omitted to implement an existing geoid model into the software-loop to reserve this workload for a later time.

\newpage
\section{Report Visualization}


An interactive data report is developed for data analytics instead of a pdf type report since it works more efficiently and rather follows the paradigma of a single source of truth since it allows for dynamic updates and thus does not represent a data source in itself but rather view on the source facilitating updates on the data.
An interactive report then needs to be able to quickly gain an overview over state of sensors. It is also an effective tool to develop FMEA implementations and accelerate development by closing the feedback loop.

%And But Therefore


\begin{figure}
    \centering
    \includegraphics{stashboard_screenshot_1}
    \caption{Stashboard webbased analytic Tool }
    \label{fig:stashboard_1}
\end{figure}
\begin{figure}
    \centering
    \includegraphics{stashboard_screenshot_2}
    \caption{Level 1,2,3}
    \label{fig:stashboard_2}
\end{figure}
\begin{figure}
    \centering
    \includegraphics{stashboard_screenshot_3}
    \caption{Level 3 Analysis}
    \label{fig:stashboard_3}
\end{figure}

\subsection{Visualization}
A main paradigm of this visualization is that it prioritizes qualitative over quantitative design. Meaning that it does not mention explicit details but allows to comprehend and retrace the qualitative FMEA results. It shall be a UX-centric design with the possibility of identifying outliers or oversensitive SHM config parameters. Level1: Quickly see if many sensors are missing Level2: Identify outliers

\paragraph{Parameter Availability}
After monitoring, a list of missing parameters is generated. To show and detect this info quickly, a speed gauge style display is chosen to display this scalar value.

\paragraph{Displaying notable occurences of single sensor examination}
To quickly get an overview of missing parameters, a timeline graph is implemented, showing cumulative errors over the flight. This allows a quick overview over all parameters.


Possible features: implement altitude graph later to contextualize sensor behaviour.

\paragraph{Examining sensor interactions}

whole flight: occurences Single sensor: mean value, residual, stdev


%\subsection{ Upload of JSON to stash}


%\subsection{ Check for errors}
It shows that while conservatively chosen ranges do not indicate many occurences, tighter limits enable more detailed monitoring.

\subsection{Adapt sensitivity and algorithms (balance false positives)}

The validation loop is driven by previously known error types (See OneNote Sensor Errors)


\section{Summary}

This parameter showed the methods used and the considerations made for the implementation of this SHM. Various methods and approaches were implemented to check for sensor faults. Next to the straight implementation, focus was put into developing solid interfaces to ease further implementation of future methods of fault detection. This applies for Levels 1, 2 as well as 3 which in this work was only minimally implemented to allow this prototype to see the light of day.





